{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Text Summarization using NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8270ac5"
      },
      "source": [
        "# **Text Summarization using Natural Language Processing**"
      ],
      "id": "c8270ac5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e9ce955"
      },
      "source": [
        "### What is text summarization?"
      ],
      "id": "2e9ce955"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cefd1a87"
      },
      "source": [
        "<p style='text-align: justify;'> \n",
        "The process of shortening a piece of text to represent only the most relevant information is known as automatic text summarization. This task usually involves reducing the volume of the original text while keeping crucial informational aspects and content meaning intact using some algorithms. Manual text summarization is a cumbersome process and can often be ridden with bias. Automating this process produces compressed text that is coherent and free from influence of the writer; it provides a strong incentive to do so for academic research and is becoming increasingly popular. There is a plethora of research papers out there for any particular topic. For anyone beginning with literature review by going through research papers or reading long news articles, automatic text summarization can prove to be very handy.\n",
        "    </p>"
      ],
      "id": "cefd1a87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f60365c0"
      },
      "source": [
        "### Why is text summarization so useful?"
      ],
      "id": "f60365c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79dfefaa"
      },
      "source": [
        "<p style='text-align: justify;'> \n",
        "Text summarising is useful for a variety of NLP tasks, including text classification, question answering, legal text summarization, news summarization, and headline generation. Furthermore, the development of summaries can be implemented into these systems as an intermediate step, reducing the document's length. For any automatic text summarizer, it is important to produce a coherent, concise and fluent summary, yet preserving the meaning of the original text.\n",
        "\n",
        "For this goal, various machine learning approaches have been presented. The majority of these solutions model this topic as a classification problem that determines whether or not a sentence should be included in the summary. TextRank algorithms, Topic information, Latent Semantic Analysis (LSA), Sequence to Sequence models, Reinforcement Learning, and Adversarial processes are all capable of this task.\n",
        "</p>"
      ],
      "id": "79dfefaa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "871f307d"
      },
      "source": [
        "### Types of text summarization"
      ],
      "id": "871f307d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b5e7291"
      },
      "source": [
        "#### Extractive text summarization:\n",
        "<p style='text-align: justify;'> \n",
        "This technique relies on picking up or extracting the most important phrases from a piece of text. This approach first finds out sentence similarity and then assigns weight on the basis of its importance. The most important sentences are returned. Here, no new sentences are generated, only existing text in the document is used in summarization process. This is relatively easier. Common approaches include TextRank algorithm, TF-IDF, latent semantic analysis, etc.\n",
        "</p>\n",
        "\n",
        "#### Abstractive text summarization:\n",
        "<p style='text-align: justify;'> \n",
        "In this technique, important parts of the document are paraphrased and shortened. New senetences are generated after the model understands the context of the document and produces its own semantics. This is a part of natural language generation and hence is more complex that the extractive approach. Deep learning produces good results in this case. Examples of common approaches include encoder-decoder models, transformer models, etc.\n",
        "</p>\n",
        "\n"
      ],
      "id": "0b5e7291"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48b6b27c"
      },
      "source": [
        "## Extractive text summarization using TF-IDF"
      ],
      "id": "48b6b27c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb9c7f75"
      },
      "source": [
        "Term frequency-Inverse Document frequency is a numerical technique that reflects how relevant a word is to the entire document. This metric that has two components, the **Term Frequency**, that represents the level of importance of a given word to all the other words in the document, and the **Inverse Document Frequency** is a measure of how unique or how common a word is in the whole document corpus. "
      ],
      "id": "cb9c7f75"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9975ad25"
      },
      "source": [
        "#### Term frequency\n",
        "\n",
        "This is a count of how many times a particular term appears in the document. Using merely the term frequency will result in an inaccurate result, as a longer document will have a higher TF for a word 'X' than a shorter document. However, dividing the frequency by the number of words can normalize this.\n",
        "\n",
        "        TF(w, d) = occurrences of word ‘w’ in the document ‘d’ / total number of words in the document ‘d’"
      ],
      "id": "9975ad25"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1964463a"
      },
      "source": [
        "#### Inverse document frequency\n",
        "\n",
        "Document frequency is the number of times the word ‘w’ is in the set of documents. It is the number of documents that contain the word.\n",
        "\n",
        "        DF(w) = number of times the word ‘w’ appears\n",
        "        \n",
        "Its inverse is more relevant, as common words will have a low IDF value while terms with a lesser occurrence gain higher precedence. Log values of this score makes it even useful. \n",
        "\n",
        "                        IDF(w) = log(N / (DF + 1))"
      ],
      "id": "1964463a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4758d7e2"
      },
      "source": [
        "Therefore,\n",
        "    \n",
        "                        TF-IDF(w, d) = TF(w, d) * log(N / (DF + 1))"
      ],
      "id": "4758d7e2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cdcdc95"
      },
      "source": [
        "#### Import libraries"
      ],
      "id": "9cdcdc95"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da7c5743"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "id": "da7c5743",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjNfeKzuvWdf",
        "outputId": "42ea67dd-e9c3-417c-c8a3-a077e25e473f"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "id": "PjNfeKzuvWdf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eabf9484"
      },
      "source": [
        "#### Clean the text"
      ],
      "id": "eabf9484"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e9f4c95"
      },
      "source": [
        "def clean_text(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        text = file.read()\n",
        "    article = sent_tokenize(text)\n",
        "    sentences = []\n",
        "    \n",
        "    # removing special characters and extra whitespaces\n",
        "    for sentence in article:\n",
        "        sentence = re.sub('[\\s+]', ' ', sentence)\n",
        "        sentences.append(sentence)\n",
        "    sentences.pop() \n",
        "    display = \"\".join(sentences)\n",
        "    print('Initial Text: ')\n",
        "    print(display)\n",
        "    print('\\n')\n",
        "    return sentences"
      ],
      "id": "7e9f4c95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ee92d9f"
      },
      "source": [
        "#### Calculate occurences of words in each sentence"
      ],
      "id": "5ee92d9f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3338ab1"
      },
      "source": [
        "# counting the number of words in the document (sentence)\n",
        "def cnt_words(sent):\n",
        "    cnt = 0\n",
        "    words = word_tokenize(sent)\n",
        "    for word in words:\n",
        "        cnt = cnt + 1\n",
        "    return cnt\n",
        "   \n",
        "# getting data about each sentence (frequency of words) \n",
        "def cnt_in_sent(sentences):\n",
        "    txt_data = []\n",
        "    i = 0\n",
        "    for sent in sentences:\n",
        "        i = i + 1\n",
        "        cnt = cnt_words(sent)\n",
        "        temp = {'id' : i, 'word_cnt' : cnt}\n",
        "        txt_data.append(temp)\n",
        "    return txt_data"
      ],
      "id": "a3338ab1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c30bee0"
      },
      "source": [
        "#### Create a list of frequencies for each word in all document"
      ],
      "id": "4c30bee0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e075715"
      },
      "source": [
        "# creating a dictionary of words for each document (sentence)\n",
        "def freq_dict(sentences):\n",
        "    i = 0\n",
        "    freq_list = []\n",
        "    for sent in sentences:\n",
        "        i = i + 1\n",
        "        freq_dict = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            if word in freq_dict:\n",
        "                freq_dict[word] = freq_dict[word] + 1\n",
        "            else:\n",
        "                freq_dict[word] = 1\n",
        "            temp = {'id' : i, 'freq_dict' : freq_dict}\n",
        "        freq_list.append(temp)\n",
        "    return freq_list"
      ],
      "id": "9e075715",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c26f15f7"
      },
      "source": [
        "#### Calculate TF and IDF values"
      ],
      "id": "c26f15f7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bf3dfc5"
      },
      "source": [
        "# calculating the term frequency \n",
        "def calc_TF(text_data, freq_list):\n",
        "    tf_scores = []\n",
        "    for item in freq_list:\n",
        "        ID = item['id']\n",
        "        for k in item['freq_dict']:\n",
        "            temp = {\n",
        "                'id': item['id'],\n",
        "                'tf_score': item['freq_dict'][k]/text_data[ID-1]['word_cnt'],\n",
        "                'key': k\n",
        "                }\n",
        "            tf_scores.append(temp)\n",
        "    return tf_scores\n",
        "\n",
        "#calculating the inverse document frequency\n",
        "def calc_IDF(text_data, freq_list):\n",
        "    idf_scores =[]\n",
        "    cnt = 0\n",
        "    for item in freq_list:\n",
        "        cnt = cnt + 1\n",
        "        for k in item['freq_dict']:\n",
        "            val = sum([k in it['freq_dict'] for it in freq_list])\n",
        "            temp = {\n",
        "                'id': cnt, \n",
        "                'idf_score': math.log(len(text_data)/(val+1)), \n",
        "                'key': k}\n",
        "            idf_scores.append(temp)\n",
        "    return idf_scores"
      ],
      "id": "8bf3dfc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "581f549a"
      },
      "source": [
        "#### Calculate TF-IDF values"
      ],
      "id": "581f549a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bbdef67"
      },
      "source": [
        "# calculating TFIDF value\n",
        "def calc_TFIDF(tf_scores, idf_scores):\n",
        "    tfidf_scores = []\n",
        "    for j in idf_scores:\n",
        "        for i in tf_scores:\n",
        "            if j['key'] == i['key'] and j['id'] == i['id']:\n",
        "                temp = {\n",
        "                    'id': j['id'],\n",
        "                    'tfidf_score': j['idf_score'] * i['tf_score'],\n",
        "                    'key': j['key']\n",
        "                    }\n",
        "                tfidf_scores.append(temp)\n",
        "    return tfidf_scores"
      ],
      "id": "6bbdef67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fa5c8d4"
      },
      "source": [
        "#### Rank the documents based on TFIDF score"
      ],
      "id": "3fa5c8d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268a7095"
      },
      "source": [
        "# giving each sentence a score\n",
        "def sent_scores(tfidf_scores, sentences, text_data):\n",
        "    sent_data = []\n",
        "    for txt in text_data:\n",
        "        score = 0\n",
        "        for i in range(0, len(tfidf_scores)):\n",
        "            t_dict = tfidf_scores[i]\n",
        "            if txt['id'] == t_dict['id']:\n",
        "                score = score + t_dict['tfidf_score']\n",
        "        temp = {\n",
        "            'id': txt['id'],\n",
        "            'score': score,\n",
        "            'sentence': sentences[txt['id']-1]}\n",
        "        sent_data.append(temp)\n",
        "    return sent_data"
      ],
      "id": "268a7095",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc5e856"
      },
      "source": [
        "#### Summarizing the text"
      ],
      "id": "9dc5e856"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6b1cf0e"
      },
      "source": [
        "def summary(sent_data):\n",
        "    cnt = 0\n",
        "    summary = []\n",
        "    for t_dict in sent_data:\n",
        "        cnt  = cnt + t_dict['score']\n",
        "    avg = cnt / len(sent_data)\n",
        "    for sent in sent_data:\n",
        "        if sent['score'] >= (avg):\n",
        "            summary.append(sent['sentence'])\n",
        "    summary = \" \".join(summary)\n",
        "    return summary"
      ],
      "id": "b6b1cf0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7eb2a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff21823-89cd-4997-b975-9d288e63d59b"
      },
      "source": [
        "sentences =  clean_text('test.txt')\n",
        "text_data = cnt_in_sent(sentences)\n",
        "\n",
        "freq_list = freq_dict(sentences)\n",
        "tf_scores = calc_TF(text_data, freq_list)\n",
        "idf_scores = calc_IDF(text_data, freq_list)\n",
        "\n",
        "tfidf_scores = calc_TFIDF(tf_scores, idf_scores)\n",
        "\n",
        "sent_data = sent_scores(tfidf_scores, sentences, text_data)\n",
        "result = summary(sent_data)\n",
        "\n",
        "print('Final Summary: ')\n",
        "print(result)"
      ],
      "id": "b7eb2a18",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Text: \n",
            "Climate change raised the risk of deadly floods that swept through Western Europe in July, new research confirms.As people around the world continue burning fossil fuels and pumping out greenhouse gas emissions, the threat of similar disasters grows.Already, single-day rainfall events in the affected region are between 3 to 19 percent more intense than they would be without human-caused climate change.Global warming also makes extreme rainfall events like the one that triggered mid-July floods in Germany, Belgium, the Netherlands, and Luxembourg between 1.2 and nine times more likely to occur in the region.At least 220 people died in Germany and Belgium because of swollen rivers and floods that raged through entire towns, knocking homes off their foundations.The disaster was triggered by heavy rainfall that smashed records as the amount of water that might normally fall over a span of a couple of months instead came down in just two days.HUMAN ACTIVITY IS THE MAIN CULPRIT BEHIND MORE EXTREME WEATHER “This event demonstrates once again in 2021 that extremes breaking observed records by far, exacerbated by climate change, can strike anywhere, induce huge damages and cause fatalities.Western Europe’s local and national authorities need to be aware of the increasing risks from extreme precipitation to be better prepared for potential future events,” Frank Kreienkamp, head of the Regional Climate Office Potsdam at the German weather service Deutscher Wetterdienst, said in a statement today from the World Meteorological Organization.Human activity is the main culprit behind the more extreme weather we’re beginning to experience, a landmark United Nations climate report found earlier this month.By burning fossil fuels, humans have unleashed greenhouse gases that heat the planet, intensifying the water cycle.That UN report aggregated the findings from hundreds of “attribution studies” that determine if climate change supercharged individual weather events.The new study on Europe’s July floods is one such attribution study, released yesterday by an international group of 39 researchers.Though their study hasn’t yet been published in a peer-reviewed journal, the researchers used established methods to quickly analyze the event.They looked back at rainfall records and modeled how much rainfall the area might have received with and without the influence of climate change.They started by homing in on two regions that were hit hard by the floods: Germany’s Ahr and Erft region and Belgium’s Meuse region.But with this particular study, the researchers came up against more challenges than they have with other attribution studies.They lacked data on river flow over time, in part because floodwaters washed away measurement stations.They also had limited long-term, local data, so they broadened their study to a larger area of Western Europe.Because of those limitations, the report gives a wide range for how much more likely climate change made the floods.EXTREME WEATHER ATTRIBUTION IS A FIELD OF STUDY THAT’S GROWING IN LEAPS AND BOUNDS Extreme weather attribution is a field of study that’s growing in leaps and bounds.Thanks to more computation power, more advanced models, and remote sensing, scientists can now tie the climate crisis to individual weather events with more speed and certainty than they were able to just a few years ago.“This is a really exciting, cutting-edge field right now,” Alex Ruane, one of the authors of the recent UN report and a research physical scientist at the NASA Goddard Institute for Space Studies, told The Verge this month.“Methodological advances and several groups that have really taken this on as a major focus of their efforts have, in many ways, increased our ability and the speed at which we can make these types of connections.So that’s a big advantage.”  Scientists will need that advantage as they study more extreme events.For now, global warming has reached about 1.2 degrees Celsius above pre-industrial levels.When it reaches 2 degrees Celsius, rainfall from a similar event in Europe could be up to 6 percent more intense, according to the attribution study.\n",
            "\n",
            "\n",
            "Final Summary: \n",
            "As people around the world continue burning fossil fuels and pumping out greenhouse gas emissions, the threat of similar disasters grows. Already, single-day rainfall events in the affected region are between 3 to 19 percent more intense than they would be without human-caused climate change. At least 220 people died in Germany and Belgium because of swollen rivers and floods that raged through entire towns, knocking homes off their foundations. The disaster was triggered by heavy rainfall that smashed records as the amount of water that might normally fall over a span of a couple of months instead came down in just two days. HUMAN ACTIVITY IS THE MAIN CULPRIT BEHIND MORE EXTREME WEATHER “This event demonstrates once again in 2021 that extremes breaking observed records by far, exacerbated by climate change, can strike anywhere, induce huge damages and cause fatalities. Western Europe’s local and national authorities need to be aware of the increasing risks from extreme precipitation to be better prepared for potential future events,” Frank Kreienkamp, head of the Regional Climate Office Potsdam at the German weather service Deutscher Wetterdienst, said in a statement today from the World Meteorological Organization. By burning fossil fuels, humans have unleashed greenhouse gases that heat the planet, intensifying the water cycle. Though their study hasn’t yet been published in a peer-reviewed journal, the researchers used established methods to quickly analyze the event. They started by homing in on two regions that were hit hard by the floods: Germany’s Ahr and Erft region and Belgium’s Meuse region. They lacked data on river flow over time, in part because floodwaters washed away measurement stations. “Methodological advances and several groups that have really taken this on as a major focus of their efforts have, in many ways, increased our ability and the speed at which we can make these types of connections. For now, global warming has reached about 1.2 degrees Celsius above pre-industrial levels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21021a"
      },
      "source": [
        "The TF-IDF model does pretty well on extractive summarizing. The TextRank algorithm and GloVe embeddings perform better in terms of coherence in the summarized text."
      ],
      "id": "af21021a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86be80d6"
      },
      "source": [
        "## Extractive text summarization using TextRank algorithm and GloVe embeddings"
      ],
      "id": "86be80d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd3bfcdd"
      },
      "source": [
        "TextRank is an extractive and unsupervised text summarization technique. It is based on the same approach as PageRank algorithm to rank webpages; here sentences are evaluated instead of webpages and similarity between sentences are determined. \n",
        "\n",
        "The process is to concatenate text in the available articles. Then, the text is separated into individual sentences. We will next find the vector representation (word embeddings) for every sentence and stored the calculated similarity in a matrix. The sentence rank is calculate and the top-ranked sentences are picked as a part of summary."
      ],
      "id": "fd3bfcdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c8d0c16"
      },
      "source": [
        "In the following application, we have a set of articles about tennis matches and we attempt to summarize them. Hence, this is a single-domain-multiple-documents summarization task."
      ],
      "id": "5c8d0c16"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaf26fe9"
      },
      "source": [
        "#### Import libraries"
      ],
      "id": "aaf26fe9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e909d006"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re"
      ],
      "id": "e909d006",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b162873"
      },
      "source": [
        "#### Read the data"
      ],
      "id": "4b162873"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bcdff4a0",
        "outputId": "cb0bc1b2-cb34-48a3-f821-c74e2d78f97a"
      },
      "source": [
        "df = pd.read_csv(\"tennis_articles.csv\", encoding='utf-8')\n",
        "\n",
        "df.head()"
      ],
      "id": "bcdff4a0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_title</th>\n",
              "      <th>article_text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I do not have friends in tennis, says Maria Sh...</td>\n",
              "      <td>Maria Sharapova has basically no friends as te...</td>\n",
              "      <td>https://www.tennisworldusa.org/tennis/news/Mar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Federer defeats Medvedev to advance to 14th Sw...</td>\n",
              "      <td>BASEL, Switzerland (AP) — Roger Federer advanc...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/copil-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Tennis: Roger Federer ignored deadline set by ...</td>\n",
              "      <td>Roger Federer has revealed that organisers of ...</td>\n",
              "      <td>https://scroll.in/field/899938/tennis-roger-fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Nishikori to face off against Anderson in Vien...</td>\n",
              "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/nishiko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Roger Federer has made this huge change to ten...</td>\n",
              "      <td>Federer, 37, first broke through on tour over ...</td>\n",
              "      <td>https://www.express.co.uk/sport/tennis/1036101...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_id  ...                                             source\n",
              "0           1  ...  https://www.tennisworldusa.org/tennis/news/Mar...\n",
              "1           2  ...  http://www.tennis.com/pro-game/2018/10/copil-s...\n",
              "2           3  ...  https://scroll.in/field/899938/tennis-roger-fe...\n",
              "3           4  ...  http://www.tennis.com/pro-game/2018/10/nishiko...\n",
              "4           5  ...  https://www.express.co.uk/sport/tennis/1036101...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f494c14e"
      },
      "source": [
        "The articles have a title and a whole document of text."
      ],
      "id": "f494c14e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abf9fa26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e8d5627b-cc55-4e19-d658-8c740ec95dc6"
      },
      "source": [
        "df['article_text'][0]  # inspecting the text in the article"
      ],
      "id": "abf9fa26",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here. When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net. So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well. Uhm, I'm not really friendly or close to many players. I have not a lot of friends away from the courts.' When she said she is not really close to a lot of players, is that something strategic that she is doing? Is it different on the men's tour than the women's tour? 'No, not at all. I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. I think every person has different interests. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life. I think everyone just thinks because we're tennis players we should be the greatest of friends. But ultimately tennis is just a very small part of what we do. There are so many other things that we're interested in, that we do.' ALSO READ: Maria Sharapova reveals how tennis keeps her motivated.\""
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a24d1754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5df8cf7d-1d31-4c70-fa93-e620a58d5f05"
      },
      "source": [
        "df['article_text'][1]"
      ],
      "id": "a24d1754",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"BASEL, Switzerland (AP) — Roger Federer advanced to the 14th Swiss Indoors final of his career by beating seventh-seeded Daniil Medvedev 6-1, 6-4 on Saturday. Seeking a ninth title at his hometown event, and a 99th overall, Federer will play 93th-ranked Marius Copil on Sunday. Federer dominated the 20th-ranked Medvedev and had his first match-point chance to break serve again at 5-1. He then dropped his serve to love, and let another match point slip in Medvedev's next service game by netting a backhand. He clinched on his fourth chance when Medvedev netted from the baseline. Copil upset expectations of a Federer final against Alexander Zverev in a 6-3, 6-7 (6), 6-4 win over the fifth-ranked German in the earlier semifinal. The Romanian aims for a first title after arriving at Basel without a career win over a top-10 opponent. Copil has two after also beating No. 6 Marin Cilic in the second round. Copil fired 26 aces past Zverev and never dropped serve, clinching after 2 1/2 hours with a forehand volley winner to break Zverev for the second time in the semifinal. He came through two rounds of qualifying last weekend to reach the Basel main draw, including beating Zverev's older brother, Mischa. Federer had an easier time than in his only previous match against Medvedev, a three-setter at Shanghai two weeks ago.\""
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7999aa3f"
      },
      "source": [
        "#### Split text into sentences"
      ],
      "id": "7999aa3f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a02f8ff2"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for s in df['article_text']:\n",
        "    sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x] # flatten list\n"
      ],
      "id": "a02f8ff2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da8f1d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c057ba54-3474-4089-8050-67949d904ef9"
      },
      "source": [
        "sentences[:5]  # checking a few sentences"
      ],
      "id": "da8f1d77",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Maria Sharapova has basically no friends as tennis players on the WTA Tour.',\n",
              " \"The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much.\",\n",
              " 'I think everyone knows this is my job here.',\n",
              " \"When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.\",\n",
              " \"So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07a61b08"
      },
      "source": [
        "#### GloVe word embeddings"
      ],
      "id": "07a61b08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fdb3f13"
      },
      "source": [
        "GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for the sentences. "
      ],
      "id": "9fdb3f13"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7383eb",
        "outputId": "74354862-1e8a-423e-8712-32f86836d069"
      },
      "source": [
        "## !wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "id": "8b7383eb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-07 12:31:37--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-09-07 12:31:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-07 12:31:37--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.11MB/s    in 2m 40s  \n",
            "\n",
            "2021-09-07 12:34:18 (5.13 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zEFkVGR6AdI",
        "outputId": "33842441-9d21-4c52-a36e-4c77371dc756"
      },
      "source": [
        "## !unzip glove*.zip"
      ],
      "id": "5zEFkVGR6AdI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFS6YhT1iwmp"
      },
      "source": [
        "#### Extract word embeddings"
      ],
      "id": "jFS6YhT1iwmp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7abf4873"
      },
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "id": "7abf4873",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO17CYADi3Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794f40d6-f6b2-4459-d219-993350622232"
      },
      "source": [
        "len(word_embeddings)"
      ],
      "id": "ZO17CYADi3Pl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW71MqB6Qq5-"
      },
      "source": [
        "#### Text preprocessing\n",
        "In this step, we try to clean our text by removing punctuations and special characters. Following this, the text is converted into lowercase and get rid of stopwords. "
      ],
      "id": "FW71MqB6Qq5-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uniiVA107s7A"
      },
      "source": [
        "# remove punctuations, numbers and special characters\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "id": "uniiVA107s7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxBlITg5ROaO",
        "outputId": "d8058f19-539b-42c3-d860-1c0f75675c00"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "id": "uxBlITg5ROaO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1EMOLrWRPy3"
      },
      "source": [
        "# function to remove stopwords\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new"
      ],
      "id": "P1EMOLrWRPy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbDrIQaZRXjB"
      },
      "source": [
        "# remove stopwords from the sentences\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "id": "vbDrIQaZRXjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A3FupZ8SZB4"
      },
      "source": [
        "#### Creating vector representations of sentences\n",
        "On generating clean sentences, we use the GloVe embeddings to create vectors for the sentences."
      ],
      "id": "3A3FupZ8SZB4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOh8tY97SWDb"
      },
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "id": "HOh8tY97SWDb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Xlc9LwTr8g"
      },
      "source": [
        "We create vectors for the constituent words in a sentence and then we take the average of those vectors to get a combined vector for the sentence."
      ],
      "id": "s5Xlc9LwTr8g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQBaX3rgTrJW"
      },
      "source": [
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)"
      ],
      "id": "yQBaX3rgTrJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnJ85nTUw8Q"
      },
      "source": [
        "#### Finding similarity between sentences\n",
        "To find the similarity between the sentences, we create a similarity matrix using the cosine similarity approach. A n*n matrix (n is the number of sentences) with all zeroes is created and is then filled with the similarity scores of sentences."
      ],
      "id": "6jnJ85nTUw8Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ANRyAEnUwot"
      },
      "source": [
        "# similarity matrix\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])"
      ],
      "id": "2ANRyAEnUwot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn1gQzFWUr2H"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "id": "Gn1gQzFWUr2H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHtNnWhZM0E"
      },
      "source": [
        "#### Applying the TextRank algorithm\n",
        "The similarity matrix is converted into a graph. The nodes of the graph represent the sentences and edges represent similarity scores between the sentences. We next apply the PageRank algorithm to obtain the sentence rankings. \n"
      ],
      "id": "ocHtNnWhZM0E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc-v7VDTZIkH"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "id": "oc-v7VDTZIkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvC58tr4a09y"
      },
      "source": [
        "#### Summary extraction\n",
        "Based on the text rankings, we select the top N sentences for summary generation."
      ],
      "id": "KvC58tr4a09y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBzuX-P8azeN",
        "outputId": "c5a5bc82-c82e-4123-aba9-f9272d2a5a07"
      },
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "# Extract top 10 sentences as the summary\n",
        "for i in range(10):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "id": "zBzuX-P8azeN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "“I was on a nice trajectorythen,” Reid recalled.“If I hadn’t got sick, I think I could have started pushing towards the second week at the slams and then who knows.” Duringa comeback attempt some five years later, Reid added Bernard Tomic and 2018 US Open Federer slayer John Millman to his list of career scalps.\n",
            "Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.\n",
            "So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "Speaking at the Swiss Indoors tournament where he will play in Sunday’s final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
            "Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month.\n",
            "Exhausted after spending half his round deep in the bushes searching for my ball, as well as those of two other golfers he’d never met before, our incredibly giving designated driver asked if we didn’t mind going straight home after signing off so he could rest up a little before heading to work.\n",
            "“I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments.\n",
            "“I just felt like it really kind of changed where people were a little bit, definitely in the '90s, a lot more quiet, into themselves, and then it started to become better.” Meanwhile, Federer is hoping he can improve his service game as he hunts his ninth Swiss Indoors title this week.\n",
            "The former Wimbledon junior champion was full of hope, excited about getting his life back together after a troubled few years and a touch-and-go battle with pancreatitis.\n",
            "He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstractive Text Summarization using attention and encoder-decoder model"
      ],
      "metadata": {
        "id": "hMscy5KS2ERW"
      },
      "id": "hMscy5KS2ERW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://www.kaggle.com/snap/amazon-fine-food-reviews"
      ],
      "metadata": {
        "id": "a3GdlHm82TDv"
      },
      "id": "a3GdlHm82TDv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyq_kmvfbJoo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "Yyq_kmvfbJoo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/content/Reviews.csv.zip\",nrows=100000)"
      ],
      "metadata": {
        "id": "I4WAuZag2pgv"
      },
      "id": "I4WAuZag2pgv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "xWx1ahyP6AxB"
      },
      "id": "xWx1ahyP6AxB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
        "data.dropna(axis=0,inplace=True)#dropping na"
      ],
      "metadata": {
        "id": "R8ej4Xqx3DeJ"
      },
      "id": "R8ej4Xqx3DeJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9N5hHZp5yIv",
        "outputId": "7e1dfc0b-8ebd-4140-cf44-5b45e97744d1"
      },
      "id": "f9N5hHZp5yIv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 88421 entries, 0 to 99999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count  Dtype \n",
            "---  ------                  --------------  ----- \n",
            " 0   Id                      88421 non-null  int64 \n",
            " 1   ProductId               88421 non-null  object\n",
            " 2   UserId                  88421 non-null  object\n",
            " 3   ProfileName             88421 non-null  object\n",
            " 4   HelpfulnessNumerator    88421 non-null  int64 \n",
            " 5   HelpfulnessDenominator  88421 non-null  int64 \n",
            " 6   Score                   88421 non-null  int64 \n",
            " 7   Time                    88421 non-null  int64 \n",
            " 8   Summary                 88421 non-null  object\n",
            " 9   Text                    88421 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 7.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "qyBA2sDF81xe"
      },
      "id": "qyBA2sDF81xe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGc-aQAz_d1q",
        "outputId": "b37d4932-e505-4d4c-dbca-ea0994d74cc9"
      },
      "id": "sGc-aQAz_d1q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "metadata": {
        "id": "b6a5yRRo85TB"
      },
      "id": "b6a5yRRo85TB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call the function\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) "
      ],
      "metadata": {
        "id": "MpiaBdeq85Rv"
      },
      "id": "MpiaBdeq85Rv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text[:5]  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOiY1lar8-r_",
        "outputId": "950120b9-8da8-45bb-f2ee-07eb30d3370c"
      },
      "id": "iOiY1lar8-r_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
              " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
              " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
              " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
              " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#call the function\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ],
      "metadata": {
        "id": "Ra1iHQsf8_6n"
      },
      "id": "Ra1iHQsf8_6n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_summary[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDTPQ-8A9BRn",
        "outputId": "20c4c2f0-ca47-40a0-88ee-a12670269b9b"
      },
      "id": "DDTPQ-8A9BRn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good quality dog food',\n",
              " 'not as advertised',\n",
              " 'delight says it all',\n",
              " 'cough medicine',\n",
              " 'great taffy',\n",
              " 'nice taffy',\n",
              " 'great just as good as the expensive brands',\n",
              " 'wonderful tasty taffy',\n",
              " 'yay barley',\n",
              " 'healthy dog food']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "metadata": {
        "id": "7aybrl1p9CWx"
      },
      "id": "7aybrl1p9CWx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "loGISxGe9DfU"
      },
      "id": "loGISxGe9DfU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "GOQqSH099E1H",
        "outputId": "0c664d4e-0643-43d7-b295-b26bccac1175"
      },
      "id": "GOQqSH099E1H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=8):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_summary']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rIUgpi49Gd2",
        "outputId": "63a99979-0cbb-42c6-bedf-a3efffab3745"
      },
      "id": "_rIUgpi49Gd2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9424907471335922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_text_len=30\n",
        "max_summary_len=8"
      ],
      "metadata": {
        "id": "CP28og4I9H5L"
      },
      "id": "CP28og4I9H5L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "metadata": {
        "id": "eUyThi4n9JQa"
      },
      "id": "eUyThi4n9JQa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "metadata": {
        "id": "f60swSIF9Klg"
      },
      "id": "f60swSIF9Klg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
      ],
      "metadata": {
        "id": "5xjzW94B9MXK"
      },
      "id": "5xjzW94B9MXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "metadata": {
        "id": "6rt5dHPT9Obf"
      },
      "id": "6rt5dHPT9Obf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXHBytwK9Ppn",
        "outputId": "4a1aa516-9db8-41d7-b497-19b157c4a936"
      },
      "id": "eXHBytwK9Ppn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 66.12339930151339\n",
            "Total Coverage of rare words: 2.953684513790566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "metadata": {
        "id": "fz2YAPDx9SCl"
      },
      "id": "fz2YAPDx9SCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_voc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpVmf5sa9TVz",
        "outputId": "71853747-ba40-4792-face-996862d70db6"
      },
      "id": "LpVmf5sa9TVz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8440"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "metadata": {
        "id": "JTojM4Kv9URE"
      },
      "id": "JTojM4Kv9URE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e-9zxVy9Vmm",
        "outputId": "a994aef8-2a5f-4997-efba-38cc3af6e3a1"
      },
      "id": "8e-9zxVy9Vmm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 78.12740675541863\n",
            "Total Coverage of rare words: 5.3921899389571895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "metadata": {
        "id": "c-8xRL7a9Xj6"
      },
      "id": "c-8xRL7a9Xj6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BapfOVo19Y_g",
        "outputId": "99f10f5a-aee6-4499-8e4d-1bf1e0447b97"
      },
      "id": "BapfOVo19Y_g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42453, 42453)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ],
      "metadata": {
        "id": "lvrfhZM19bHn"
      },
      "id": "lvrfhZM19bHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "metadata": {
        "id": "-Kn0463r9cfu"
      },
      "id": "-Kn0463r9cfu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n"
      ],
      "metadata": {
        "id": "reh45Ohz9eAO"
      },
      "id": "reh45Ohz9eAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zX_ZFvcpHqZL"
      },
      "id": "zX_ZFvcpHqZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "-Cmr32OU9n47",
        "outputId": "ceb37e4c-08a6-4486-9a0c-d53e29ab1a27"
      },
      "id": "-Cmr32OU9n47",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-42daacc329ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n"
      ],
      "metadata": {
        "id": "N3tGbG6y9sN-"
      },
      "id": "N3tGbG6y9sN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
      ],
      "metadata": {
        "id": "WO4vzmmW9tX5"
      },
      "id": "WO4vzmmW9tX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "ERTfZ1BI9vBp"
      },
      "id": "ERTfZ1BI9vBp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "metadata": {
        "id": "WR2D_aWI9woV"
      },
      "id": "WR2D_aWI9woV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "metadata": {
        "id": "0gD4A4yf9x4i"
      },
      "id": "0gD4A4yf9x4i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "YqcHndbH9z01"
      },
      "id": "YqcHndbH9z01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "metadata": {
        "id": "hH5cjW9h92M1"
      },
      "id": "hH5cjW9h92M1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "0lg0DLuO949W"
      },
      "id": "0lg0DLuO949W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NT7WOih98GG"
      },
      "id": "7NT7WOih98GG",
      "execution_count": null,
      "outputs": []
    }
  ]
}